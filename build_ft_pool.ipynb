{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune the CLIP model on Flickr30K, CoCo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    CLIPProcessor, \n",
    "    CLIPModel,\n",
    "    EarlyStoppingCallback,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from zeroshot_retrieval import zeroshot_evaluate\n",
    "from tqdm import tqdm\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_path = \"openai/clip-vit-base-patch16\"\n",
    "save_path = \"saved_clip_model\\\\clip-vit-base-p16-finetuned_proj_512x512\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    'nlphuji/flickr30k',\n",
    "    cache_dir='cache',\n",
    "    keep_in_memory=True,\n",
    ")['test'] # type: ignore\n",
    "\n",
    "train_dataset = dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "test_dataset = dataset.filter(lambda x: x[\"split\"] == \"test\")\n",
    "val_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(train_dataset, \"data/flickr30k/train_dataset.pt\")\n",
    "# torch.save(test_dataset, \"data/flickr30k/test_dataset.pt\")\n",
    "# torch.save(val_dataset, \"data/flickr30k/val_dataset.pt\")\n",
    "\n",
    "train_dataset = torch.load(\"data/flickr30k/train_dataset.pt\")\n",
    "val_dataset = torch.load(\"data/flickr30k/val_dataset.pt\")\n",
    "test_dataset = torch.load(\"data/flickr30k/test_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(pretrained_model_path, cache_dir=\"cache\").cuda()\n",
    "# model.visual_projection = nn.Linear(in_features=768, out_features=768, bias=False)\n",
    "# model.text_projection = nn.Linear(in_features=512, out_features=768, bias=False)\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(pretrained_model_path, cache_dir=\"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = []\n",
    "def tokenize_function(examples, max_seq_length=77):\n",
    "    # First, obtain the sentences. \n",
    "    # Here, one image corresponds to five sentences, and we will combine them into one sentence\n",
    "    # tmp.append(examples)\n",
    "    text = [\" \".join('%s' %a for a in sentence) for sentence in examples['caption']]\n",
    "\n",
    "    image = [image.convert(\"RGB\") for image in examples['image']]\n",
    "    processed_data = processor(\n",
    "        images=image, \n",
    "        text=text,\n",
    "        padding='max_length',\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        return_attention_mask=True, \n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    batch = {\n",
    "        'input_ids':processed_data['input_ids'],\n",
    "        # 'token_type_ids':processed_data['token_type_ids'],\n",
    "        'attention_mask':processed_data['attention_mask'],\n",
    "        'pixel_values':processed_data['pixel_values'],\n",
    "    }\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenlizing\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=True, \n",
    "        remove_columns=['image', 'split','caption','sentids', 'img_id', 'filename'], \n",
    "        # keep_in_memory = True,\n",
    "        batch_size = 500,\n",
    "    )\n",
    "torch.save(tokenized_train_dataset, \"tokenized_dataset/flickr30k_maxseqlen_77_512x512_p16/tokenized_train_batched_500.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7baf95c82efe489e9f5a59e589e3932a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_eval_dataset = val_dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=True, \n",
    "        remove_columns=['image', 'split','caption','sentids', 'img_id', 'filename'],\n",
    "        # keep_in_memory = True,\n",
    "        batch_size = 500,\n",
    "    )\n",
    "\n",
    "torch.save(tokenized_eval_dataset, \"tokenized_dataset/flickr30k_maxseqlen_77_512x512_p16/tokenized_eval_batched_500.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_dataset = test_dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=True, \n",
    "        remove_columns=['image', 'split','caption','sentids', 'img_id', 'filename'],\n",
    "        # keep_in_memory = True,\n",
    "        batch_size = 500,\n",
    "    )\n",
    "torch.save(tokenized_test_dataset, \"tokenized_dataset/flickr30k_maxseqlen_77_512x512_p16/tokenized_test_batched_500.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = torch.load(\"tokenized_dataset/flickr30k_maxseqlen_77_512x512_p16/tokenized_train_batched_500.pt\")\n",
    "tokenized_eval_dataset = torch.load(\"tokenized_dataset/flickr30k_maxseqlen_77_512x512_p16/tokenized_eval_batched_500.pt\")\n",
    "tokenized_test_dataset = torch.load(\"tokenized_dataset/flickr30k_maxseqlen_77_512x512_p16/tokenized_test_batched_500.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'pixel_values'],\n",
       "    num_rows: 29000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tmp = []\n",
    "# # tmp1 = []\n",
    "# def collate_fn_epoch(examples):  # evaluation_strategy = \"epoch\"\n",
    "#     pixel_values = torch.stack([torch.tensor(example[\"pixel_values\"]) for example in examples], dim=0)\n",
    "#     input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "#     attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long)\n",
    "#     return {\n",
    "#         \"pixel_values\": pixel_values,\n",
    "#         \"input_ids\": input_ids,\n",
    "#         \"attention_mask\": attention_mask,\n",
    "#         \"return_loss\": True,\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_steps(examples):  # evaluation_strategy = \"steps\"\n",
    "    pixel_values = torch.stack([torch.tensor(example[\"pixel_values\"]) for example in examples], dim=0).squeeze(0)\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long).squeeze(1)\n",
    "    attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long).squeeze(1)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"return_loss\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #FREEZE\n",
    "# for i,j in model.named_parameters():\n",
    "#     if 'prompt' not in i:\n",
    "#         j.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStoppingCallback(early_stopping_patience=10)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir            = save_path,\n",
    "    evaluation_strategy   = \"steps\",\n",
    "    learning_rate         = 3e-6,\n",
    "    weight_decay          = 1e-6,\n",
    "    save_steps            = 50,\n",
    "    eval_steps            = 50,\n",
    "    num_train_epochs      = 20,\n",
    "    save_strategy         = \"steps\",\n",
    "    remove_unused_columns = False,\n",
    "    warmup_steps          = 50,\n",
    "    per_device_train_batch_size = 128,\n",
    "    per_device_eval_batch_size  = 128,\n",
    "    lr_scheduler_type     = \"cosine\",\n",
    "    label_smoothing_factor= 0.05,\n",
    "    # auto_find_batch_size  = True,\n",
    "    metric_for_best_model = \"eval_loss\",  # for earlystop\n",
    "    # metric_for_best_model = \"eval_acc\",  # for earlystop\n",
    "    load_best_model_at_end = True,\n",
    "    save_total_limit = 1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn_steps,\n",
    "    callbacks=[earlystop],\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "results = zeroshot_evaluate(model=model,dataloader=test_dataset,processor=processor,max_seq_length=77,recall_k_list=[1,5,10],device=torch.device(\"cuda\")) # type: ignore\n",
    "\n",
    "ic(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [01:03, 15.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'image_retrieval_recall@1': 0.7523999810218811,\n",
       " 'text_retrieval_recall@1': 0.8970000147819519,\n",
       " 'image_retrieval_recall@5': 0.9343999624252319,\n",
       " 'text_retrieval_recall@5': 0.9880000352859497,\n",
       " 'image_retrieval_recall@10': 0.9666000008583069,\n",
       " 'text_retrieval_recall@10': 0.9930000305175781}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For KeyboardInterrupt usage\n",
    "test_dataset = torch.load(\"data/flickr30k/test_dataset.pt\")\n",
    "zeroshot_evaluate(model=model,dataloader=test_dataset,processor=processor,max_seq_length=77,recall_k_list=[1,5,10],device=torch.device(\"cuda\")) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot Cross-Modal Retrieval on Flickr30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeroshot_retrieval import zeroshot_evaluate\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from transformers import (\n",
    "    CLIPProcessor, \n",
    "    CLIPModel,\n",
    "    CLIPConfig,\n",
    ")\n",
    "\n",
    "test_dataset = torch.load(\"data/flickr30k/test_dataset.pt\")\n",
    "pretrained_model_path = \"openai/clip-vit-base-patch32\"\n",
    "save_path = \"saved_clip_model\\\\clip-vit-base-p32-finetuned_proj_768x768\"\n",
    "\n",
    "config = CLIPConfig().from_pretrained(save_path)\n",
    "\n",
    "# if 512x512\n",
    "# model = CLIPModel(config).from_pretrained(save_path, cache_dir=\"cache\").cuda()\n",
    "\n",
    "# if 768x768\n",
    "model = CLIPModel(config).cuda()\n",
    "model.visual_projection = torch.nn.Linear(in_features=768, out_features=768, bias=False).cuda()\n",
    "model.text_projection = torch.nn.Linear(in_features=512, out_features=768, bias=False).cuda()\n",
    "pretrained_dict = torch.load(save_path+'\\\\pytorch_model.bin', map_location=device)\n",
    "model.load_state_dict(pretrained_dict)\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(pretrained_model_path, cache_dir=\"cache\")\n",
    "\n",
    "\n",
    "results = zeroshot_evaluate(model=model,dataloader=test_dataset,processor=processor,max_seq_length=77,recall_k_list=[1,5,10],device=torch.device(\"cuda\")) # type: ignore\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Feature Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    CLIPProcessor, \n",
    "    CLIPModel,\n",
    "    AutoConfig,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# pretrained_model_path = \"openai/clip-vit-base-patch32\"\n",
    "# save_path = \"output\\\\clip-finetuned_proj_768x768\"\n",
    "pretrained_model_path = \"openai/clip-vit-base-patch16\"\n",
    "save_path = \"saved_clip_model\\\\clip-vit-base-p16-finetuned_proj_512x512\"\n",
    "\n",
    "# dataset = load_dataset('nlphuji/flickr30k',cache_dir='cache',keep_in_memory=False,)['test'] \n",
    "# train_dataset = dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "train_dataset = torch.load(\"data/flickr30k/train_dataset.pt\")\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(pretrained_model_path, cache_dir=\"cache\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(save_path)\n",
    "model = CLIPModel(config).from_pretrained(save_path, cache_dir=\"cache\").cuda()\n",
    "\n",
    "# model = CLIPModel(config).cuda()\n",
    "# model.visual_projection = torch.nn.Linear(in_features=768, out_features=768, bias=False)\n",
    "# model.text_projection = torch.nn.Linear(in_features=512, out_features=768, bias=False)\n",
    "# pretrained_dict = torch.load(save_path+\"/pytorch_model.bin\")\n",
    "# model_dict = model.state_dict()\n",
    "# model_dict.update(pretrained_dict)\n",
    "# model.load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "def tokenize_func(examples, max_seq_length=77):\n",
    "    # tmp.append(examples)\n",
    "    text = [\" \".join('%s' %a for a in sentence) for sentence in examples['caption']]\n",
    "\n",
    "    image = [image.convert(\"RGB\") for image in examples['image']]\n",
    "    processed_data = processor(\n",
    "        images=image, \n",
    "        text=text,\n",
    "        padding='max_length',\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        return_attention_mask=True, \n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenlizing\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "        tokenize_func, \n",
    "        batched=True, \n",
    "        remove_columns=['image', 'split','caption','sentids', 'img_id', 'filename'], \n",
    "        # keep_in_memory = True,\n",
    "        batch_size = 500,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tokenized_train_dataset, \"tokenized_dataset/flickr30k_maxseqlen_77_512x512_p16/tokenized_train_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_train_dataset = torch.load(\"tokenized_dataset/flickr30k_maxseqlen_77_512x512_p16/tokenized_train_dataset.pt\")\n",
    "pixel_values = torch.stack([torch.tensor(example[\"pixel_values\"]) for example in tokenized_train_dataset], dim=0).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([example[\"input_ids\"] for example in tokenized_train_dataset], dtype=torch.long).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.tensor([example[\"attention_mask\"] for example in tokenized_train_dataset], dtype=torch.long).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pixel_values, \"tokenized_dataset/flickr30k_maxseqlen_77_512x512_p16/pixel_values.pt\")\n",
    "torch.save(input_ids, \"tokenized_dataset/flickr30k_maxseqlen_77_512x512_p16/input_ids.pt\")\n",
    "torch.save(attention_mask, \"tokenized_dataset/flickr30k_maxseqlen_77_512x512_p16/attention_mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = torch.load(\"tokenized_dataset/flickr30k_maxseqlen_77_512x512_p16/pixel_values.pt\")\n",
    "input_ids = torch.load(\"tokenized_dataset/flickr30k_maxseqlen_77_512x512_p16/input_ids.pt\")\n",
    "attention_mask = torch.load(\"tokenized_dataset/flickr30k_maxseqlen_77_512x512_p16/attention_mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29000, 3, 224, 224])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29000, 77])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29000it [09:44, 49.63it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_images_emb_list = []\n",
    "batch_texts_emb_list = []\n",
    "model.cuda()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    ret_dict = {}\n",
    "    for pvalue, ids, atm in tqdm(zip(pixel_values, input_ids, attention_mask)):\n",
    "        ret_dict = model(pixel_values=pvalue.unsqueeze(0).cuda(), \n",
    "                        input_ids=ids.unsqueeze(0).cuda(),\n",
    "                        attention_mask=atm.unsqueeze(0).cuda(),\n",
    "                        output_hidden_states=True,\n",
    "                        )\n",
    "        batch_images_emb_list.append(ret_dict['image_embeds'])    # [1,512] \n",
    "        batch_texts_emb_list.append(ret_dict['text_embeds'])      # [1,512] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_texts_emb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_flickr30k_trainset_img_features = torch.stack(batch_images_emb_list, dim=0).squeeze(1)\n",
    "all_flickr30k_trainset_text_features = torch.stack(batch_texts_emb_list, dim=0).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29000, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_flickr30k_trainset_img_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(all_flickr30k_testset_img_features, \"data/flickr30k/all_flickr30k_testset_img_features.pt\")\n",
    "# torch.save(all_flickr30k_testset_text_features, \"data/flickr30k/all_flickr30k_testset_text_features.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(all_flickr30k_trainset_img_features, \"data/flickr30k/all_flickr30k_trainset_img_features.pt\")\n",
    "# torch.save(all_flickr30k_trainset_text_features, \"data/flickr30k/all_flickr30k_trainset_text_features.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_flickr30k_trainset_img_features, \"data/flickr30k/flickr30k_trainset_img_features_512x512_p16.pt\")\n",
    "torch.save(all_flickr30k_trainset_text_features, \"data/flickr30k/flickr30k_trainset_text_features_512x512_p16.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
